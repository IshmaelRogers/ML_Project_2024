{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IshmaelRogers/ML_Project_2024/blob/main/mlproj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8Rw1SDqWbm7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Constants\n",
        "sampling_rate = 100  # Hz\n",
        "duration = 400  # seconds\n",
        "time = np.arange(0, duration, 1/sampling_rate)\n",
        "\n",
        "# Function to generate synthetic IMU data\n",
        "def generate_trajectory(amplitude, frequency, phase_shift):\n",
        "    return amplitude * np.sin(2 * np.pi * frequency * time + phase_shift)\n",
        "\n",
        "# Simulate different motion patterns\n",
        "amplitudes = [1, 2, 0.5, 1.5, 2.5, 3]\n",
        "frequencies = [0.1, 0.2, 0.15, 0.05, 0.3, 0.25]\n",
        "phase_shifts = [0, np.pi/4, np.pi/2, 3*np.pi/4, np.pi, 5*np.pi/4]\n",
        "\n",
        "# Generate trajectories\n",
        "trajectories = []\n",
        "for amp, freq, phase in zip(amplitudes, frequencies, phase_shifts):\n",
        "    trajectory = generate_trajectory(amp, freq, phase)\n",
        "    trajectories.append(trajectory)\n",
        "\n",
        "# Stack trajectories for accelerometer (3-axis) and gyroscope (3-axis)\n",
        "accel_data = np.vstack((trajectories, trajectories, trajectories)).T\n",
        "gyro_data = np.vstack((trajectories, trajectories, trajectories)).T\n",
        "\n",
        "# Add noise characteristics\n",
        "def add_noise(data, noise_level):\n",
        "    noise = noise_level * np.random.randn(*data.shape)\n",
        "    return data + noise\n",
        "\n",
        "noise_levels = np.linspace(0.001, 0.025, 15)\n",
        "noisy_datasets = []\n",
        "ground_truth_labels = []\n",
        "\n",
        "for noise_level in noise_levels:\n",
        "    noisy_accel_data = add_noise(accel_data, noise_level)\n",
        "    noisy_gyro_data = add_noise(gyro_data, noise_level)\n",
        "    noisy_datasets.append((noisy_accel_data, noisy_gyro_data))\n",
        "    ground_truth_labels.append(noise_level)\n",
        "\n",
        "# Combine all datasets and ground truth labels\n",
        "data = np.vstack([np.hstack((acc, gyro)) for acc, gyro in noisy_datasets])\n",
        "labels = np.repeat(ground_truth_labels, accel_data.shape[0], axis=0)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "data = scaler.fit_transform(data)\n",
        "\n",
        "# Save the dataset\n",
        "output_dir = \"simulated_datasets\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "np.save(os.path.join(output_dir, \"data.npy\"), data)\n",
        "np.save(os.path.join(output_dir, \"labels.npy\"), labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfwDPcNlyh2V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KaEShQ7b7-_",
        "outputId": "c5e204b8-1bb3-442a-9ae0-1dfc39602c1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape: (40000, 18)\n",
            "Data contents:\n",
            " [[-1.58128165e-03  1.41420565e+00  4.99502553e-01 ...  1.06109692e+00\n",
            "  -1.83406368e-03 -2.12161112e+00]\n",
            " [ 6.26383895e-03  1.43161120e+00  4.99766879e-01 ...  1.05693954e+00\n",
            "  -4.78414260e-02 -2.15215245e+00]\n",
            " [ 1.06939478e-02  1.44895221e+00  5.00316744e-01 ...  1.05284044e+00\n",
            "  -9.46603346e-02 -2.18762413e+00]\n",
            " ...\n",
            " [-1.82250696e-02  1.35844734e+00  5.00043968e-01 ...  1.07055178e+00\n",
            "   1.41389526e-01 -2.01960533e+00]\n",
            " [-1.09408095e-02  1.37689902e+00  4.97840151e-01 ...  1.06555959e+00\n",
            "   9.53505535e-02 -2.05371363e+00]\n",
            " [-5.17364875e-03  1.39573595e+00  4.99268336e-01 ...  1.06465506e+00\n",
            "   4.81040437e-02 -2.08899516e+00]]\n",
            "Data contents (first 10 rows):\n",
            " [[-1.58128165e-03  1.41420565e+00  4.99502553e-01  1.06259141e+00\n",
            "  -2.77054266e-03 -2.12046681e+00  7.00260194e-04  1.41442079e+00\n",
            "   5.00333292e-01  1.05999189e+00 -3.70575663e-05 -2.12316244e+00\n",
            "  -9.56215401e-04  1.41523602e+00  5.02403347e-01  1.06109692e+00\n",
            "  -1.83406368e-03 -2.12161112e+00]\n",
            " [ 6.26383895e-03  1.43161120e+00  4.99766879e-01  1.05844854e+00\n",
            "  -4.69691572e-02 -2.15317526e+00  7.03454571e-03  1.43287203e+00\n",
            "   5.00510103e-01  1.05793633e+00 -4.72772128e-02 -2.15400795e+00\n",
            "   6.31254486e-03  1.43249629e+00  5.00291355e-01  1.05693954e+00\n",
            "  -4.78414260e-02 -2.15215245e+00]\n",
            " [ 1.06939478e-02  1.44895221e+00  5.00316744e-01  1.05251002e+00\n",
            "  -9.40800603e-02 -2.18683208e+00  1.05396014e-02  1.44833912e+00\n",
            "   5.00713540e-01  1.05390305e+00 -9.55916019e-02 -2.18528073e+00\n",
            "   1.23392968e-02  1.44727896e+00  4.97867554e-01  1.05284044e+00\n",
            "  -9.46603346e-02 -2.18762413e+00]\n",
            " [ 1.92397268e-02  1.46621825e+00  5.00805943e-01  1.05027794e+00\n",
            "  -1.41286729e-01 -2.21895189e+00  1.87635434e-02  1.46538977e+00\n",
            "   4.98213756e-01  1.05168036e+00 -1.42420567e-01 -2.21866867e+00\n",
            "   1.67707907e-02  1.46484343e+00  4.99391014e-01  1.05166945e+00\n",
            "  -1.40479845e-01 -2.21764013e+00]\n",
            " [ 2.41112082e-02  1.48381663e+00  4.99819923e-01  1.04619863e+00\n",
            "  -1.89628501e-01 -2.25126992e+00  2.51472507e-02  1.48233323e+00\n",
            "   5.00385536e-01  1.04748586e+00 -1.87887053e-01 -2.24767028e+00\n",
            "   2.47163140e-02  1.48444688e+00  4.99052780e-01  1.04768010e+00\n",
            "  -1.88368597e-01 -2.25084838e+00]\n",
            " [ 3.17154307e-02  1.50084735e+00  5.00334014e-01  1.04221251e+00\n",
            "  -2.35454378e-01 -2.28142252e+00  3.15633489e-02  1.50011494e+00\n",
            "   4.98335569e-01  1.04363813e+00 -2.36223408e-01 -2.28303815e+00\n",
            "   3.28036863e-02  1.50074700e+00  4.97203496e-01  1.04483292e+00\n",
            "  -2.35125027e-01 -2.28213009e+00]\n",
            " [ 3.85291684e-02  1.51861649e+00  4.99503795e-01  1.04203965e+00\n",
            "  -2.83685097e-01 -2.31162472e+00  3.67016291e-02  1.51678619e+00\n",
            "   4.99952888e-01  1.04028879e+00 -2.81898673e-01 -2.31346862e+00\n",
            "   3.73800039e-02  1.51662497e+00  4.99470020e-01  1.03874020e+00\n",
            "  -2.82363626e-01 -2.31172610e+00]\n",
            " [ 4.29572177e-02  1.53303594e+00  5.00530957e-01  1.03740911e+00\n",
            "  -3.28993193e-01 -2.34109860e+00  4.59131895e-02  1.53204434e+00\n",
            "   4.98992314e-01  1.03537597e+00 -3.29462278e-01 -2.34235715e+00\n",
            "   4.37551106e-02  1.53376406e+00  4.99004343e-01  1.03711747e+00\n",
            "  -3.28746599e-01 -2.34091642e+00]\n",
            " [ 5.13969613e-02  1.54816722e+00  4.96062613e-01  1.03484545e+00\n",
            "  -3.75217127e-01 -2.37108588e+00  5.07459673e-02  1.54762967e+00\n",
            "   4.97986750e-01  1.03295755e+00 -3.76354561e-01 -2.37041080e+00\n",
            "   5.08090237e-02  1.54996830e+00  4.98839601e-01  1.03239812e+00\n",
            "  -3.76797310e-01 -2.36876949e+00]\n",
            " [ 5.52463895e-02  1.56475714e+00  4.97634106e-01  1.02963446e+00\n",
            "  -4.21356342e-01 -2.39717819e+00  5.78565108e-02  1.56485867e+00\n",
            "   4.99582108e-01  1.03042462e+00 -4.22217648e-01 -2.40219978e+00\n",
            "   5.55571145e-02  1.56396416e+00  4.98090333e-01  1.03056481e+00\n",
            "  -4.22871529e-01 -2.39776153e+00]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load the .npy file\n",
        "file_path = \"simulated_datasets/noisy_accel_data_0.npy\"  #\n",
        "data = np.load(file_path)\n",
        "\n",
        "# Display the contents\n",
        "print(\"Data shape:\", data.shape)\n",
        "print(\"Data contents:\\n\", data)\n",
        "\n",
        "#\n",
        "print(\"Data contents (first 10 rows):\\n\", data[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivs9pPO0X8Mg",
        "outputId": "9e2e8069-35ee-46c8-9a8c-915257f0e9d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculated sequence length: 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/30], Training Loss: 0.0003, Validation Loss: 0.0001, Learning Rate: 0.001000\n",
            "Epoch [2/30], Training Loss: 0.0001, Validation Loss: 0.0001, Learning Rate: 0.001000\n",
            "Epoch [3/30], Training Loss: 0.0001, Validation Loss: 0.0001, Learning Rate: 0.001000\n",
            "Epoch [4/30], Training Loss: 0.0001, Validation Loss: 0.0001, Learning Rate: 0.001000\n",
            "Epoch [5/30], Training Loss: 0.0001, Validation Loss: 0.0001, Learning Rate: 0.001000\n",
            "Epoch [6/30], Training Loss: 0.0001, Validation Loss: 0.0001, Learning Rate: 0.001000\n",
            "Epoch [7/30], Training Loss: 0.0001, Validation Loss: 0.0001, Learning Rate: 0.001000\n",
            "Epoch [8/30], Training Loss: 0.0001, Validation Loss: 0.0001, Learning Rate: 0.001000\n",
            "Epoch [9/30], Training Loss: 0.0001, Validation Loss: 0.0001, Learning Rate: 0.001000\n",
            "Epoch [10/30], Training Loss: 0.0001, Validation Loss: 0.0001, Learning Rate: 0.001000\n",
            "Epoch [11/30], Training Loss: 0.0001, Validation Loss: 0.0001, Learning Rate: 0.001000\n",
            "Epoch [12/30], Training Loss: 0.0000, Validation Loss: 0.0000, Learning Rate: 0.001000\n",
            "Epoch [13/30], Training Loss: 0.0000, Validation Loss: 0.0000, Learning Rate: 0.001000\n",
            "Epoch [14/30], Training Loss: 0.0000, Validation Loss: 0.0000, Learning Rate: 0.001000\n",
            "Epoch [15/30], Training Loss: 0.0000, Validation Loss: 0.0000, Learning Rate: 0.001000\n",
            "Epoch [16/30], Training Loss: 0.0000, Validation Loss: 0.0000, Learning Rate: 0.001000\n",
            "Epoch [17/30], Training Loss: 0.0000, Validation Loss: 0.0000, Learning Rate: 0.001000\n",
            "Epoch [18/30], Training Loss: 0.0000, Validation Loss: 0.0000, Learning Rate: 0.001000\n",
            "Epoch [19/30], Training Loss: 0.0000, Validation Loss: 0.0000, Learning Rate: 0.001000\n",
            "Epoch [20/30], Training Loss: 0.0000, Validation Loss: 0.0000, Learning Rate: 0.001000\n",
            "Epoch [21/30], Training Loss: 0.0000, Validation Loss: 0.0000, Learning Rate: 0.001000\n",
            "Epoch [22/30], Training Loss: 0.0000, Validation Loss: 0.0000, Learning Rate: 0.001000\n",
            "Epoch [23/30], Training Loss: 0.0000, Validation Loss: 0.0000, Learning Rate: 0.001000\n",
            "Epoch [24/30], Training Loss: 0.0000, Validation Loss: 0.0000, Learning Rate: 0.001000\n",
            "Epoch [25/30], Training Loss: 0.0000, Validation Loss: 0.0000, Learning Rate: 0.001000\n",
            "Epoch [26/30], Training Loss: 0.0000, Validation Loss: 0.0000, Learning Rate: 0.001000\n",
            "Epoch [27/30], Training Loss: 0.0000, Validation Loss: 0.0000, Learning Rate: 0.001000\n",
            "Epoch [28/30], Training Loss: 0.0000, Validation Loss: 0.0000, Learning Rate: 0.001000\n",
            "Epoch [29/30], Training Loss: 0.0000, Validation Loss: 0.0000, Learning Rate: 0.001000\n",
            "Epoch [30/30], Training Loss: 0.0000, Validation Loss: 0.0000, Learning Rate: 0.001000\n",
            "Training complete\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# DNN Class with Adjusted Kernel Sizes and Dropout\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=6, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc1 = nn.Linear(256, 100)\n",
        "        self.fc2 = nn.Linear(100, 80)\n",
        "        self.fc3 = nn.Linear(80, 50)\n",
        "        self.fc4 = nn.Linear(50, 20)\n",
        "        self.fc5 = nn.Linear(20, 6)  # 6 output values for process noise variances\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = self.fc5(x)  # No activation in the final layer\n",
        "        return x\n",
        "\n",
        "# Helper function to create data loaders\n",
        "def create_data_loaders(train_data, train_labels, val_data, val_labels, batch_size):\n",
        "    train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_data, dtype=torch.float32),\n",
        "                                                   torch.tensor(train_labels, dtype=torch.float32))\n",
        "    val_dataset = torch.utils.data.TensorDataset(torch.tensor(val_data, dtype=torch.float32),\n",
        "                                                 torch.tensor(val_labels, dtype=torch.float32))\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, val_loader\n",
        "\n",
        "# Function to train the DNN with Learning Rate Finder\n",
        "def train_dnn(model, train_loader, val_loader, num_epochs=30, initial_lr=0.001):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.view(inputs.size(0), 6, -1)  # Change shape to (batch_size, num_channels, sequence_length)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs = inputs.view(inputs.size(0), 6, -1)  # Change shape to (batch_size, num_channels, sequence_length)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {running_loss/len(train_loader):.4f}, Validation Loss: {val_loss/len(val_loader):.4f}, Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "    print('Training complete')\n",
        "    return model\n",
        "\n",
        "# Load the simulated data and ground truth\n",
        "output_dir = \"simulated_datasets\"\n",
        "noisy_datasets = []\n",
        "ground_truth_labels = []\n",
        "\n",
        "for i in range(15):\n",
        "    noisy_accel_data = np.load(os.path.join(output_dir, f\"noisy_accel_data_{i}.npy\"))\n",
        "    noisy_gyro_data = np.load(os.path.join(output_dir, f\"noisy_gyro_data_{i}.npy\"))\n",
        "    ground_truth = np.load(os.path.join(output_dir, f\"ground_truth_{i}.npy\"))\n",
        "    noisy_data = np.hstack((noisy_accel_data, noisy_gyro_data))\n",
        "    noisy_datasets.append(noisy_data)\n",
        "    ground_truth_labels.append(ground_truth)\n",
        "\n",
        "# Combine all datasets\n",
        "data = np.vstack(noisy_datasets)\n",
        "labels = np.vstack(ground_truth_labels)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "data = scaler.fit_transform(data.reshape(-1, data.shape[-1])).reshape(data.shape)\n",
        "\n",
        "# Ensure sequence length is appropriate\n",
        "sequence_length = data.shape[1] // 6\n",
        "print(f\"Calculated sequence length: {sequence_length}\")\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.25, random_state=42)\n",
        "\n",
        "# Reshape data to ensure correct input dimensions\n",
        "train_data = train_data.reshape(train_data.shape[0], 6, -1)\n",
        "val_data = val_data.reshape(val_data.shape[0], 6, -1)\n",
        "test_data = test_data.reshape(test_data.shape[0], 6, -1)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 500\n",
        "train_loader, val_loader = create_data_loaders(train_data, train_labels, val_data, val_labels, batch_size)\n",
        "\n",
        "# Initialize and train the DNN\n",
        "model = DNN()\n",
        "model = train_dnn(model, train_loader, val_loader, num_epochs=30, initial_lr=0.001)\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'dnn_model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKOMnvSlUO5H",
        "outputId": "371535fc-1560-4528-9c05-3531980af321"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated State:\n",
            "Position:    [0.66740699 0.66740699 0.65656733]\n",
            "Velocity:    [0.0110496  0.0110496  0.33732334]\n",
            "Orientation: [ 0.00889504  0.00889504 -0.03373233]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class HybridAdaptiveESKF:\n",
        "    def __init__(self, state_dim, control_dim, measurement_dim):\n",
        "        self.state_dim = state_dim\n",
        "        self.control_dim = control_dim\n",
        "        self.measurement_dim = measurement_dim\n",
        "\n",
        "        self.x_n = np.zeros(state_dim)          # Nominal state\n",
        "        self.delta_x = np.zeros(state_dim)      # Error state\n",
        "        self.P = np.eye(state_dim)              # Error covariance matrix\n",
        "        self.Q = np.eye(state_dim)              # Process noise covariance\n",
        "        self.R = np.eye(measurement_dim)        # Measurement noise covariance\n",
        "\n",
        "    def predict(self, u, f, F, G, dt):\n",
        "        # Predict nominal state\n",
        "        self.x_n = f(self.x_n, u, dt)\n",
        "\n",
        "        # Predict error covariance\n",
        "        F_matrix = F(self.x_n, u, dt)\n",
        "        G_matrix = G(self.x_n, u, dt)\n",
        "        self.P = F_matrix @ self.P @ F_matrix.T + G_matrix @ self.Q @ G_matrix.T\n",
        "\n",
        "    def update(self, z, h, H):\n",
        "        # Measurement residual\n",
        "        y = z - h(self.x_n)\n",
        "\n",
        "        # Kalman gain\n",
        "        H_matrix = H(self.x_n)\n",
        "        S = H_matrix @ self.P @ H_matrix.T + self.R\n",
        "        K = self.P @ H_matrix.T @ np.linalg.inv(S)\n",
        "\n",
        "        # Update error state\n",
        "        self.delta_x = K @ y\n",
        "\n",
        "        # Update error covariance\n",
        "        self.P = (np.eye(self.state_dim) - K @ H_matrix) @ self.P\n",
        "\n",
        "        # Correct nominal state\n",
        "        self.x_n = self.x_n + self.delta_x\n",
        "\n",
        "    def set_process_noise(self, Q):\n",
        "        self.Q = Q\n",
        "\n",
        "    def set_measurement_noise(self, R):\n",
        "        self.R = R\n",
        "\n",
        "    def get_state(self):\n",
        "        return {\n",
        "            'position': self.x_n[:3],\n",
        "            'velocity': self.x_n[3:6],\n",
        "            'orientation': self.x_n[6:9]\n",
        "        }\n",
        "\n",
        "# Example usage with INS data:\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the system dimensions\n",
        "    state_dim = 9  # Example: [position (3), velocity (3), orientation (3)]\n",
        "    control_dim = 6  # Example: [acceleration (3), angular velocity (3)]\n",
        "    measurement_dim = 6  # Example: [position (3), orientation (3)]\n",
        "\n",
        "    # Instantiate the filter\n",
        "    eskf = HybridAdaptiveESKF(state_dim, control_dim, measurement_dim)\n",
        "\n",
        "    # Define the system functions\n",
        "    def f(x, u, dt):\n",
        "        # State transition function for INS data\n",
        "        F = np.eye(state_dim)\n",
        "        F[:3, 3:6] = dt * np.eye(3)\n",
        "        F[3:6, 6:9] = dt * np.eye(3)\n",
        "\n",
        "        # State transition with control input (IMU data)\n",
        "        x_new = F @ x\n",
        "        x_new[3:6] += u[:3] * dt  # Update velocity with acceleration\n",
        "        x_new[6:9] += u[3:6] * dt  # Update orientation with angular velocity\n",
        "\n",
        "        return x_new\n",
        "\n",
        "    def F(x, u, dt):\n",
        "        # Jacobian of the state transition function\n",
        "        F_matrix = np.eye(state_dim)\n",
        "        F_matrix[:3, 3:6] = dt * np.eye(3)\n",
        "        F_matrix[3:6, 6:9] = dt * np.eye(3)\n",
        "        return F_matrix\n",
        "\n",
        "    def G(x, u, dt):\n",
        "        # Process noise influence\n",
        "        return np.eye(state_dim)\n",
        "\n",
        "    def h(x):\n",
        "        # Measurement function (e.g., GPS providing position and orientation)\n",
        "        return x[:measurement_dim]\n",
        "\n",
        "    def H(x):\n",
        "        # Jacobian of the measurement function\n",
        "        return np.eye(measurement_dim, state_dim)\n",
        "\n",
        "    # Simulate a step with INS data\n",
        "    u = np.array([0.0, 0.0, 9.81, 0.1, 0.1, 0.0])  # [ax, ay, az, wx, wy, wz]\n",
        "    z = np.array([1.0, 1.0, 1.0, 0.0, 0.0, 0.0])  # [px, py, pz, ox, oy, oz]\n",
        "    dt = 0.1  # Time step\n",
        "\n",
        "    # Prediction step\n",
        "    eskf.predict(u, f, F, G, dt)\n",
        "\n",
        "    # Update step\n",
        "    eskf.update(z, h, H)\n",
        "\n",
        "    # Get the estimated state\n",
        "    state = eskf.get_state()\n",
        "\n",
        "    # Print the estimated state with labels\n",
        "    print(\"Estimated State:\")\n",
        "    print(f\"Position:    {state['position']}\")\n",
        "    print(f\"Velocity:    {state['velocity']}\")\n",
        "    print(f\"Orientation: {state['orientation']}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1m74JKUPfXFzHrKlGsKIIFUky7OtZtccN",
      "authorship_tag": "ABX9TyN7ZufkM0kC/bi1yRfIcjMn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}